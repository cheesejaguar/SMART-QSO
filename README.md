# ğŸš€ SMART-QSO (Selfâ€‘Managing Amateur Radio Transponder with Quantum State Optimization) ğŸ›°ï¸

An open, communityâ€‘driven **1U CubeSat** mission that experiments with onboard **agentic AI** to manage an amateur radio transponder. The satellite learns power availability and regional demand, prioritizes fair access, and composes dynamic beacons â€” while always **failing gracefully** âœ… to a conventional transponder if AI is unavailable.

**Key technologies**: ğŸ§  TinyML/TinyLM, âš¡ Jetson Orin Nano Super (declocked, DVFSâ€‘limited), ğŸ”Œ FreeRTOS OBC, ğŸ“¡ UHF/VHF transponder, ğŸ¤ federated learning via open amateur uplinks, ğŸ”‹ powerâ€‘aware scheduling.

> This repository contains the public mission docs, flight/ground software stubs, and hardware interface notes. See [`docs/pdr/SMART-QSO_PDR.md`](docs/pdr/SMART-QSO_PDR.md) for the NASAâ€‘style PDR.

## Status
- ğŸ“ˆ Stage: Preâ€‘Phase B â†’ Phase B (PDR complete, CDR in preparation)
- ğŸ“œ License: **MIT**
- ğŸ¤ Community: Issues and PRs welcome. Please read [CONTRIBUTING](CONTRIBUTING.md) and [CODE_OF_CONDUCT](CODE_OF_CONDUCT.md).

## Quick Start
```bash
# clone and enter
 git clone https://github.com/cheesejaguar/SMART-QSO.git
 cd smart-qso

# build (host) the flight skeleton to run unit tests
 cmake -S software/flight -B build && cmake --build build
 ctest --test-dir build
```

## Repository Structure
- ğŸ“š `docs/` architecture, ML, RF plan, regulatory notes, and the PDR.
- ğŸ”© `hardware/` mechanical/electrical notes and BOMs (MIT).
- ğŸ§° `software/` flight OBC tasks (FreeRTOS), Jetson payload stubs, ground tools, simulations.
- ğŸ—ºï¸ `mission/` ConOps, V&V, schedule, and risk register.

## ğŸ¤– Edge LLM for Health Analysis (llama.cpp, 3B primary)
- Purpose: The payload analyzes OBC health fields (as produced by `software/flight/src/main.c` and `sensors.yaml`) to generate a short, firstâ€‘person status line for the beacon. We use an offâ€‘theâ€‘shelf Llamaâ€‘3.2â€‘3B model via llama.cpp on a declocked Jetson, with strict ASCII/â‰¤120â€‘byte output and a template fallback.
- Lowest joules/token typically comes from:
  1) 4â€“5â€‘bit weight quantization that preserves quality (GGUF `Q4_K_M` â†’ `Q5_K_M` if needed)
  2) Full accelerator offload (CUDA/Metal/HIP) to avoid CPUâ†”GPU transfers
  3) Quantized KVâ€‘cache (`q8_0` safe; `q4_0` for more savings after quality checks)
  4) Speculative decoding with a 1B draft
  5) Tight runtime settings (batch/ubatch/threads/context) + prompt caching

### ğŸ§° Practical recipe
- Quantization: `Q4_K_M` (primary). Consider `Q5_K_M` if quality needs a small bump.
- Run endâ€‘toâ€‘end on the accelerator (all layers on GPU where VRAM allows).
- KVâ€‘cache quant: start `q8_0`; test `q4_0` once stable.
- Speculative: 3B main + 1B draft (e.g., `Llamaâ€‘3.2â€‘1B Q2_K/Q3_K`).

NVIDIA (CUDA) ğŸ–¥ï¸
```bash
# build (oneâ€‘time)
cmake -B build -DLLAMA_CUBLAS=ON -DCMAKE_BUILD_TYPE=Release && cmake --build build -j

# run server (full GPU offload, kvâ€‘cache quantized, healthy batching)
./build/bin/llama-server \
  -m /models/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  --gpu-layers 999 \
  --ctx-size 4096 \
  --batch-size 512 --ubatch-size 64 \
  --cache-type-k q8_0 --cache-type-v q8_0
```

Apple Silicon (Metal) ğŸ
```bash
cmake -B build -DLLAMA_METAL=ON -DCMAKE_BUILD_TYPE=Release && cmake --build build -j

./build/bin/llama-server \
  -m /models/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  --gpu-layers 999 \
  --ctx-size 4096 \
  --batch-size 512 --ubatch-size 64 \
  --cache-type-k q8_0 --cache-type-v q8_0
```

âš¡ Speculative decoding (big energy saver)
```bash
./build/bin/llama-speculative \
  -m  /models/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  -md /models/Llama-3.2-1B-Instruct-Q2_K.gguf \
  --gpu-layers 999 --draft-gpu-layers 999 \
  --ctx-size 4096 --batch-size 512 --ubatch-size 64
```

Python (llama-cpp-python) with KVâ€‘cache quant + speculative ğŸ
```python
from llama_cpp import Llama
from llama_cpp.llama_speculative import LlamaPromptLookupDecoding

ll = Llama(
    model_path="Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    n_ctx=4096, n_gpu_layers=99,
    type_k="q8_0", type_v="q8_0",  # KV cache quantization
)
draft = LlamaPromptLookupDecoding(ll)  # or a 1B draft model
out = ll.create_completion("CALL=SMARTQ-1 TIME=2025-08-18Z MODE=ACTIVE SOC=78 SUN=1 RF=1 PWR=GOOD QSO=12",
                           draft_model=draft)
```

ğŸ§­ Tuning checklist (energy first)
- Quant choice: `Q4_K_M` â†’ test â†’ `Q5_K_M` if quality dips; avoid `Q8_0` unless needed
- Offload all layers to GPU if possible (minimizes transfers)
- KVâ€‘cache quant: start `q8_0`, then evaluate `q4_0`
- Batch/ubatch: increase until tokens/s plateaus, keep headroom to avoid OOM
- Speculative decoding with a tiny draft (1B)
- Prompt discipline: smaller contexts + cached system prompts

ğŸ”‹ Measuring J/token quickly
- Log power while generating (e.g., `nvidia-smi --query-gpu=power.draw --format=csv -l 1`) and divide integrated wattâ€‘seconds by tokens emitted (llama.cpp prints tokens/s). Tune batch/ubatch/cache types to minimize J/token.

## ğŸ›¡ï¸ Governance & Safety
- Open operations under **Amateurâ€‘Satellite Service**. All frames are **in the clear** (no encryption).
- â€œFederatedâ€ updates are **bounded**, rateâ€‘limited, sandboxed, and publicly logged (see `docs/BEACON_SPEC.md`).

## ğŸ“– Citing This Work
See [CITATION.cff](CITATION.cff).
